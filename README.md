# CNN from Scratch: A Deep Learning Implementation Journey

A production-grade Convolutional Neural Network implementation using **pure NumPy**, achieving **98.33% test accuracy** on MNIST without any deep learning frameworks.

## ğŸ¯ Project Overview

This project implements a complete CNN training pipeline from first principles, including:
- Custom layer implementations (Conv, MaxPool, FC, ReLU, Flatten, Softmax)
- Vectorized operations using im2col and stride tricks
- Multiple optimizers (SGD, SGD+Momentum, Adam)
- Gradient verification system
- Full training and evaluation pipeline

**Final Results:**
- **Test Accuracy: 98.33%**
- **Validation Accuracy: 98.15%** (best epoch)
- **Training Loss: 0.0456** (final epoch)

---

## ğŸ’¡ Motivation: No Vibe-Coding

This project follows a **"zero vibe-coding"** philosophy:
- Every line of code is understood and justified
- Mathematical foundations are explored before implementation
- Gradient checking verifies correctness at each layer
- Progressive optimization (naive â†’ vectorized)

The goal: **understand CNNs at the lowest level**, not just use them.

---

## ğŸ—ï¸ Architecture

```
Input (1, 28, 28)
    â†“
Conv2D(1â†’32, 5Ã—5, padding=2) + ReLU + MaxPool(2Ã—2)
    â†“
Conv2D(32â†’64, 5Ã—5, padding=2) + ReLU + MaxPool(2Ã—2)
    â†“
Flatten (3136)
    â†“
FC(3136â†’128) + ReLU
    â†“
FC(128â†’10) + Softmax
    â†“
Cross-Entropy Loss
```

**Total Parameters:** ~455K

---

## ğŸš€ Key Technical Achievements

### 1. Mathematical Correctness
Every layer passed numerical gradient checking with error < 1e-7:
```
FC Layer:   max_error=4.93e-11 â†’ PASSED
Conv Layer: max_error=9.74e-12 â†’ PASSED
```

### 2. Performance Optimization

**Naive Implementation â†’ Vectorized**

| Operation | Naive | Vectorized | Speedup |
|-----------|-------|------------|---------|
| Conv Forward | 7 nested loops | im2col + matmul | ~15-20x |
| MaxPool Forward | 4 nested loops | as_strided + axis ops | ~10-15x |

**im2col Technique:**
```
Windows: (batchÃ—out_HÃ—out_W, in_channelsÃ—kÃ—k)
Filters: (in_channelsÃ—kÃ—k, out_channels)
Output:  (batchÃ—out_HÃ—out_W, out_channels)  â† Single matmul!
```

**Stride Tricks (MaxPool):**
```python
windows = as_strided(x, shape=(B,C,H',W',p,p), strides=(...))
output = windows.max(axis=(4,5))  # Vectorized max over all windows
```

### 3. Training Dynamics

**Loss Curve:**
```
Epoch  1: 1.2785 â†’ Epoch 20: 0.0456 (97% reduction)
```

**Accuracy Progression:**
```
Epoch  1: 86.30% val
Epoch  5: 95.90% val
Epoch 10: 97.47% val
Epoch 14: 98.08% val (best)
Epoch 20: 97.62% val
Test:     98.33%
```

### 4. Model Generalization

**Confusion Matrix Analysis:**
- Diagonal dominance: Strong per-class performance
- Hardest confusion: 4â†’9 (11 errors), 9â†’4 (8 errors)
- Best performance: Class 1 (99.6% accuracy)
- Near-perfect: Classes 0, 6, 7 (>97% each)

---

## ğŸ“ Project Structure

```
cnn-from-scratch/
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ base_layer.py          # Abstract layer interface
â”‚   â”œâ”€â”€ conv.py                # Naive convolution
â”‚   â”œâ”€â”€ conv_vec.py            # Vectorized convolution (im2col)
â”‚   â”œâ”€â”€ maxpool.py             # Naive max pooling
â”‚   â”œâ”€â”€ maxpool_vec.py         # Vectorized pooling (as_strided)
â”‚   â”œâ”€â”€ fc.py                  # Fully connected layer
â”‚   â”œâ”€â”€ relu.py                # ReLU activation
â”‚   â”œâ”€â”€ flatten.py             # Reshape layer
â”‚   â””â”€â”€ softmax.py             # Softmax + Cross-Entropy
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ base_optimizer.py      # Abstract optimizer
â”‚   â”œâ”€â”€ sgd.py                 # Vanilla SGD
â”‚   â”œâ”€â”€ momentum.py            # SGD + Momentum
â”‚   â””â”€â”€ adam.py                # Adam optimizer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base_model.py          # Model interface
â”‚   â””â”€â”€ cnn.py                 # CNN architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ gradient_check.py      # Numerical gradient verification
â”‚   â”œâ”€â”€ metrics.py             # Accuracy, confusion matrix
â”‚   â””â”€â”€ visualization.py       # Training curves, predictions
â”œâ”€â”€ data/
â”‚   â””â”€â”€ load_mnist.py          # Data loading & preprocessing
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_fc.py             # FC layer gradient check
â”‚   â””â”€â”€ test_conv.py           # Conv layer gradient check
â”œâ”€â”€ train.py                    # Training loop
â”œâ”€â”€ evaluate.py                 # Test set evaluation
â”œâ”€â”€ inference.py                # Single image prediction
â””â”€â”€ checkpoints/                # Saved models
```

---

## ğŸ”¬ Implementation Deep Dive

### Forward Pass: im2col Transformation

**Problem:** Nested loops make convolution O(nâ·) in Python.

**Solution:** Transform windows into columns, use BLAS-optimized matmul.

```python
# Extract all windows as rows
col = im2col(input, kernel_size, stride, padding)
# Shape: (batchÃ—out_HÃ—out_W, in_channelsÃ—kÃ—k)

# Reshape filters as columns
W_col = weights.reshape(out_channels, -1).T
# Shape: (in_channelsÃ—kÃ—k, out_channels)

# Single matrix multiplication replaces 7 loops
output = col @ W_col
# Shape: (batchÃ—out_HÃ—out_W, out_channels)
```

### Backward Pass: Chain Rule via Matmul

```python
# Gradient w.r.t. weights
dW = col.T @ dout_col  # Accumulate over all windows

# Gradient w.r.t. input
dX_col = dout_col @ W_col
dX = col2im(dX_col, input_shape, ...)  # Scatter back to image
```

### MaxPool: Memory-Efficient Window Views

```python
# Create 6D view without copying data
strides = (batch_stride, channel_stride, 
           strideÃ—row_stride, strideÃ—col_stride,
           row_stride, col_stride)
windows = as_strided(x, shape=(B,C,H',W',p,p), strides=strides)

# Vectorized max over last 2 dims
output = windows.max(axis=(4,5))
```

### Numerical Gradient Checking

Verify analytical gradients against finite differences:

```python
numerical = (f(Î¸ + h) - f(Î¸ - h)) / (2h)
analytical = backprop(Î¸)
relative_error = |analytical - numerical| / (|analytical| + |numerical|)

âœ… Pass: error < 1e-7
âš ï¸  Acceptable: error < 1e-5
âŒ Fail: error > 1e-3
```

---

## ğŸ“Š Training Results

### Loss & Accuracy Curves

```
Training Loss:
1.28 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.05
     â†“ Smooth exponential decay

Validation Accuracy:
86.3% â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 98.2%
      â†‘ Steady improvement, no overfitting
```

### Convergence Analysis

- **Fast initial learning:** 86% â†’ 95% in 5 epochs
- **Fine-tuning phase:** 95% â†’ 98% over 15 epochs
- **No overfitting:** Train and val curves track closely
- **Stable plateau:** Best model at epoch 14, minimal variance after

### Per-Class Performance

| Digit | Accuracy | Common Errors |
|-------|----------|---------------|
| 0 | 98.9% | â†’ 8 (2 cases) |
| 1 | 99.6% | â†’ 2 (1 case) |
| 2 | 96.6% | â†’ 9 (9 cases) |
| 3 | 99.3% | â†’ 5 (6 cases) |
| 4 | 98.4% | â†’ 9 (11 cases) |
| 5 | 98.0% | â†’ 3 (3 cases) |
| 6 | 98.8% | â†’ 0 (6 cases) |
| 7 | 99.4% | â†’ 2 (8 cases) |
| 8 | 97.7% | â†’ 5 (8 cases) |
| 9 | 96.4% | â†’ 4 (8 cases) |

**Insight:** Digits with similar strokes (4â†”9, 3â†”5, 7â†”2) show highest confusion.

---

## ğŸ› ï¸ How to Run

### Prerequisites
```bash
python >= 3.8
numpy >= 1.21.0
matplotlib >= 3.4.0
torchvision (MNIST download only)
```

### Installation
```bash
git clone https://github.com/yourusername/cnn-from-scratch.git
cd cnn-from-scratch
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip3 install -r requirements.txt
```

### Training
```bash
python3 train.py
```

**Output:**
```
Epoch 1/20 | Loss: 1.2785 | Val Acc: 0.8630
Best model saved (val_acc: 0.8630)
...
Epoch 20/20 | Loss: 0.0456 | Val Acc: 0.9762
Test Accuracy: 0.9833
```

### Evaluation
```bash
python evaluate.py  # Confusion matrix & test metrics
python inference.py  # Single image prediction
```

### Gradient Checking
```bash
python tests/test_fc.py
python tests/test_conv.py
```

---

## ğŸ”® Future Improvements

### Features to Add
- [ ] Batch Normalization
- [ ] Dropout regularization
- [ ] Data augmentation (rotation, translation)
- [ ] Learning rate scheduling
- [ ] More architectures (ResNet blocks, deeper networks)

### Optimization
- [ ] Full im2col without any loops (even spatial)
- [ ] CUDA/GPU support via CuPy
- [ ] Mixed precision training
- [ ] Depthwise separable convolutions

### Experiments
- [ ] CIFAR-10/100 dataset
- [ ] Transfer learning experiments
- [ ] Pruning and quantization
- [ ] Adversarial robustness testing

---

## ğŸ“š What I Learned

### Technical Skills
- **Deep understanding of backpropagation:** Not just chain rule, but how it flows through each layer type
- **Numerical stability:** Softmax overflow, gradient clipping, weight initialization
- **Vectorization techniques:** im2col, stride tricks, einsum operations
- **Debugging neural networks:** Gradient checking, shape tracking, loss curve analysis

### Engineering Practices
- **Test-driven development:** Gradient checks before full training
- **Progressive optimization:** Naive â†’ verified â†’ optimized
- **Memory management:** View vs copy, cache strategy
- **Modular design:** Abstract base classes, clean interfaces

### Mathematical Insights
- **Convolution as matmul:** Spatial operations â†’ linear algebra
- **He initialization:** Why variance scaling matters for ReLU
- **Adam's bias correction:** Initial steps need special handling
- **Cross-entropy + softmax gradient:** Beautiful cancellation to `p - y`

---

## ğŸ™ Acknowledgments

**Educational Resources:**
- Stanford CS231n: Convolutional Neural Networks for Visual Recognition
- "Neural Networks and Deep Learning" by Michael Nielsen
- NumPy documentation and stride tricks guide

**Inspiration:**
- Andrej Karpathy's "micrograd" philosophy
- Yann LeCun's original LeNet architecture
- "Implementing CNNs from Scratch" (various blog posts)

---

## ğŸ“ˆ Results Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Model Performance                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Test Accuracy:        98.33%           â”‚
â”‚  Validation Accuracy:  98.15%  (best)   â”‚
â”‚  Training Loss:        0.0456  (final)  â”‚
â”‚  Total Parameters:     ~455K            â”‚
â”‚  Training Time:        ~20 epochs       â”‚
â”‚  Gradient Check:       âœ… All passed    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Status:** âœ… Production-grade CNN implementation completed. No frameworks, pure understanding.

---

## ğŸ“ License

MIT License - Feel free to use this for learning and educational purposes.

---

## ğŸ‘¤ Author

**Emin Ã‡aÄŸan ApaydÄ±n** - Computer Engineering Student, Istanbul Okan University
- Focus: Computer Vision, Robotics, Deep Learning
- Project: CNN From Scratch - A deep dive into the inner workings of convolutional neural networks, built from the ground up with NumPy.

*"Understanding every line, no vibe-coding."*

---

**Star â­ this repo if you found it helpful!**